---
title: "Text Mining Delta Airlines Tweets from 2015"
author: "P. Adames"
date: "May 21, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Set Options
options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL', 'C')

library(stringi)
library(stringr)
library(qdap)
library(curl)
library(tm)

source("Helpers.R")
```


# Analysis from a clean data file of Delta Airlines

This text mining exercise of a curated sample of 15 days of Tweets from October 2015 
from the Delta Airlines Customer Service team 
looks to answer a few relevant questions:

1. What is the average length of the customer service reply in this social media platform?
2. What links were referenced most often?
3. How many social media responses per customer service representative are reasonable?
4. How many team members are needed for a similar size of operation?

Some of these can be addressed without the full data mining preprocessing, actually
the dates are useful for identifying weekly patterns for the user interaction load through
this platform. This could be the case if one wanted to go deeper into question 3 to assess
the workforce required for weekends in contrast to weekdays.
These questions can be answered before doing the preparation and cleaning steps, however,
for the sake of completeness and comparison, those steps are shown below first.

# Preparing and Cleaning the data

The data for these exercises in text mining can be found in [Ted Kwartler's Github repo](https://github.com/kwartler/text_mining).
The tweets are curated by carefully filtering those for the month of October, 2015. Their date is part of the information collected.
It's a good idea to fork the original repo and use the forked version in case the original author takes his repo down.

## Step 1

The most straight forward approach to fetching the data directly from a Github repository 
and reading it into a comma separated text file, is to use the curl library through R.
The main advantage is that curl identifies the type of content and parses it correctly without
much intervention from the user.
Care should be taken to reference the **raw** version of the text file, instead of the html rendition
of it that the hosting website provides to display the information in a human readable form through web browsers.


```{r import data}
text.df <- read.csv(curl("https://raw.githubusercontent.com/padames/text_mining/master/oct_delta.csv"), header = T, stringsAsFactors = F)
```
```{r data stats, echo=FALSE}
print(paste0("Size of data frame read in KB: ", object.size(text.df)/1000))
print(paste0("Number of rows and columns in data frame: (", nrow(text.df), ",", ncol(text.df), ")"))
```
The columns in this data frame are five: `r names(text.df)`. Only the last one is required to continue doing the
text mining of the tweets. 
Thus, a data frame with only the columns `doc_id` and `text` is created to use as input for the
functions in the `tm` package.

```{r data frame for text mining}
df <- data.frame(doc_id = seq(1:nrow(text.df)), # a numeric index
                 text = text.df$text) # the vector of 1377 tweets
```

## Step 2

The latest version of `tm`, 0.7-6, changed the way the virtual corpus is created by eliminating the 
need for a reader closure to be provided with information on the columns to be parsed as the index and the actual text.
Now, provided the data frame has the names `doc_id` and `text`, the function will be able to extarct the 
information correctly.

```{r corpus creation}
corpus <- VCorpus(DataframeSource(df))
```

## Step 3

The corpus is cleaned up using auxiliary functions found in the 
external file `Helpers.R` which is sourced at the beginning of this file.

```{r cleanup}
corpus.clean <- clean.corpus(corpus)
```

To test that we really clean the corpus a simple test is run, you can try different values of `tweet_number` between 1 and 1377:

```{r test cleanup}
tweet_number = 1045
not_cleaned <- corpus[tweet_number][[1]]$content
cleaned <- corpus.clean[tweet_number][[1]]$content
test_of_equality <- !(identical( cleaned, not_cleaned))
```
The tests give the following results for tweet number `r tweet_number`:
```{r test cleanup result, echo=F}
print( paste0("Tweet '", tweet_number, "' was cleaned: ", test_of_equality))
print(paste0("tweet['", tweet_number, "'] original; ", not_cleaned))
print(paste0("tweet['", tweet_number, "'] cleaned; ", cleaned))
```

# Analysis 

Here are answers from doing some text digging.

## What is the average length of the customer service reply in this social media platform?

Assuming that each tweet represents one single interaction with a customer then the average length of each 
represents the answer to this question. Tweet limits the length of each response to a maximum of 280 characters ([Tweeter increases length][ref1]).

Working on the original Tweets, inspection of the data frame shows that the column named `text` has all the tweets.
Since it is a vector of strings the vectorized funcion `nchar` can be used to read the length in characters of each
tweet and return a vector of those values. After that calculating their mean is straight forward:

```{r average length of a tweet}
round(x = mean(nchar(text.df$text)), digits = 0)
```

Now for comparison let's look at the average number of characters after cleaning the tweets for bag of words text mining:

```{r average length after cleaning}
clean.dataframe <- data.frame(text = unlist(sapply(corpus.clean, `[`, "content")))
round(mean(nchar(clean.dataframe[,1])), digits = 0)
```
A substantial reduction in verbosity but not as good an indicator of how much it is needed to communicate with humans using Tweeter.


## What links were referenced most often?

Ted Kwartler suggests on page 35 of his [book][ref2] that references encompass a phone number or a web site.
These would be ways to refer a customer to contact the Customer Service at Delta directly, or to visit a FAQ
page on the Delta Airlines website.

```{r references used most ofeten}
tweets.with.phone.numbers <- sum(grepl('[0-9]{3}|[0-9]{4}', text.df$text, ignore.case = T))
rounded.phone.number.freq <- round(tweets.with.phone.numbers / nrow(text.df) * 100L, digits = 2)
paste0("Phone number frequency (%): ", rounded.phone.number.freq)
tweets.with.web.sites <- sum(grepl('http', text.df$text, ignore.case = T))
rounded.web.site.freq <- round(tweets.with.web.sites / nrow(text.df) * 100L, digits = 2)
paste0("Web site frequency (%): ", rounded.web.site.freq)
ratio.phone.to.web <- round(rounded.phone.number.freq / rounded.web.site.freq, digits = 1)
paste0("Phone number to web site ratio: ", ratio.phone.to.web) 
```

This customer service agent cohort is `r ratio.phone.to.web` times more likely to give clients a phone number to address or follow up their concerns than to refer them to a web site page to do the same.


## How many social media responses per customer service representative are reasonable?

For this Ted Kwartler suggests on page 32 of his [book][ref2], using a function to detect the last two characters of every tweet, hoping to capture the initials used for each agent. Here his code to extract the last two letters of the tweet but used over the entire cleaned data frame:
```{r responses per agent with Kwartler code}
last.chars <- function(text, num) {
  last <- substr(text, nchar(text) - num + 1, nchar(text))
  return(last)
}
tbl1 <- table(Filter(x = last.chars(clean.dataframe[,1], 2), min = 2))
tbl1[order(-tbl1)]
```

Here the last word of the whole tweet is extracted using `stri_extract_last_words` from the `stringi` package.
This hopefully covers the case of variable number of letters or non-standard initials. 

```{r responses per agent}
initials <- stri_extract_last_words(clean.dataframe[,1])
tbl2 <- table(Filter(x = initials, min = 2, max = 2))
tbl2[order(-tbl2)]
```

The average response per agent over 15 days is `r round(mean(tbl1),0)` with Kwartler's function and `r round(mean(tbl2),0)` with the new code using library `stringi`'s function. Either way agent `pl` seems to be the hardest working in the group with 104 responses over this period. If especific work loads needed to be analyzed on particular peridos of time like weekends then the original data frame needs to be subset based on the time window to be considered. 

Some tweets are continuations of previous ones and they end with `\2` or `\3` to indicate the sequence after the agent's two-letter signature, this would require further examination to see how it affects the averages computed here. Numbers and other symbols are removed in the clean up procedure so this would have to be done on the original tweets.


## How many team members are needed for a similar size of operation?

According to Kwartler calculations there are `r length(tbl1)` agents to process `r sum(tbl1) - 368` interactions through Tweeter. The new code indicated `r length(tbl2)` agents processing `r sum(tbl2) - 208` interactions over the 15 days of data availble.


[ref1]: https://bgr.com/2018/02/08/twitter-character-limit-280-vs-140-user-engagement/ "Tweeter increases length"
[ref2]: https://www.amazon.com/Text-Mining-Practice-Ted-Kwartler/dp/1119282012 