---
title: "Sentiment Scoring"
author: "P. Adames"
date: "June 23, 2019"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Set Options
options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL', 'C')

library(dplyr)
library(stringi)
library(stringr)
library(qdap)
library(curl)
library(ggplot2)
library(ggthemes)
library(tm)
library(Unicode)
library(here) # read https://www.tidyverse.org/articles/2017/12/workflow-vs-script/

# package "here" assumes project file is in the root directory for relative paths to work
setwd(here("Chapter_4_Sentiment_Scoring"))

# helper functions
source(here("Chapter_2_Basics_of_Text_Mining", "Helpers.R"))
``` 

# What is sentiment analysis?

From Chapter 4 of the book: `Sentiment analysis is the process of extracting an author's emotional intent from text`. This entails using liguistics, psychology and natural languaje processing.
It is also a challenge to agree on the multiplicity of emotions and their cultural interpretation so there is always some degree of modelling/or analyst bias unless the specific intentions of the author are captured directly. Another reason for difficulty are contrasting sentiments on the same feature.

Among the numerous emotional frameworks there is one we will use created by a psychologist by the name Robert Plutchik in the 80's. It consists of eight primary emotions supposed to be essential for the survival of men and animals. Each has a polar opposite and can be expressed with different levels of intensity. Besides this other emotions can be expressed as a blend of two or more of these basic eigth.

## Basic emotions in Plutchik' emotion model

In parenthesis the polar opposites:

1. Anger (Joy)
2. Fear (Anger)
3. Sadness (Joy)
4. Disgust (Trust)
5. Surprise (Anticipation)
6. Anticipation (Surprise)
7. Trust (Disgust)
8. Joy (Sadness)

The complexity of using sentiment analysis becomes aparent considering that there
are many other frameworks for defining emotions. The main sourc of ambiguity and bias comes
from labeling a training set of n-grams and emotions from a particular sentiment framework.

## Polarity

It is simpler to analyse a text for sentiment polarity which in addition to identifying 
a sentiment it states whether it is positive or negative.

Three packages will be used in the exercises that follow:

Package             |  Use
--------------------|------------------------------------------------------------------
`sentiment`         | Basic sentiment analysis
`qdap`              | Polarity analysis
`tidytext`          | Sentiment scoring



# Sentiment Analysis: Parlor trick or insightful?

The result of sentiment scoring must be complemented with domain knowledge to make it useful in a business case. Otherwise the risk is accurate sentiment descriptions that don't lead to actionable decisions to improve a product or service. Just stating that a result is positive or negative does not make it useful.


_Example:_ Identify the positive and negative qualities of an Airbnb porperty in the Boston area.

Steps:

1. **Define problem and specific goal:** what property qualities are listed in positive/negative comments
2. **Identify the text thah needs to be collected:** the comments left by users of properties
3. **Organize text:** Select 1000 random Airbnb listing in the Boston area. Clean and organize in a frequency matrix
4. **Extract features:** Calculate sentiment and polarity scores
5. **Analyze:** Subset the comments according to sentiment and polarity in order to analyze the terms used for positive and negative comments.
6. **Make an insight or recommendation:**  Answering question formulated in step 1 will tell you if a property has the qualities considered positive in the Boston Area.


# Polarity: Simple sentiment scoring

Polarity is the positive or negative intent in the writer's tone. 
Here we will use the polarity function in the `qdap` package to do a simple but surprisingly accurate assessment of this condition. The result can be customized to specific needs by adjusting the subjectivity lexicon.

## Subjectivity lexicon

This is a list of words associated with a particular emotional state. The `qdap` function uses a peer-reviewd subjectivity lexicon from Bing Liu at the University of Illinois, consisting of 6,800 labeled words. 

A net polarity score can be calculated for a sentence by adding the positive or negative value of each word as per the subjectivity lexicon. The result is a negative, neutral, or positive tone for the excerpt. However this approach has to also consider amplifier and negaion words, so that an expression like "this is no good" results in a net negative value instead of neutral.

The lexicon can be customized to include terms specific to the text under analysis, to account for regionalisms, mannerisms, and so on.

Zipf's law states that a word will be used inversely proportional to its rank in the frequency list. That is, the number of times the fifth most frequent word will apear in a corpus with N terms will be approximately N/5.

This is explained by the least effort exerted by humans when trying to convey meaning in spite of the size of their vocabulary. A complicated message will promote disengagement from the other part, so a well researched lexicon will capture those most commonly used words relatively easily.


## `qdap` scoring for positive and negative word choice

The function works by identifying cluster of words and scoring magnifiers with +0.8 and negations with -0.8. Negative and positive words receive -1 or +1 respectively.
Additional Airbnb terms usually appearing in the comments must be added to the lexicon for more relevant results.

```{r polarity using custom lexicon terms for Airbnb}
# vector of new additional positive terms
new.pos <- c('rofl', 'lol') # rolling on the floor laughing and laugh out loud
all.pos <- c(new.pos, positive.words) # original and new positive terms
new.neg <- c('kappa','meh') # negative Airbnb terms for properties in Boston (from search)
all.neg <- c(new.neg, negative.words)  # original and new negative terms
all.polarity <- sentiment_frame(all.pos, all.neg)
polarity('ROFL, look at that!', polarity.frame = all.polarity)
```


Compare this polarity with the one obtained using the standard lexicon:

```{r polarity using standard lexicon}
polarity('ROFL, look at that!')
```

This proves the need to include terms specific to the text under study.


The same applies to the sentence `whatever you say, kappa`. Using the custom lexicon one gets a polarity result:

```{r second example of polarity using custom lexicon}
polarity('whatever you say, kappa', polarity.frame = all.polarity)
```

But using the standard lexicon:

```{r second example of polarity using standard lexicon}
polarity('whatever you say, kappa')
```

## Revisiting word clouds - Sentiment word clouds


Polarity analysis can be used to create two new corpora out of a single one and then study the terms that make positive or negative word clouds, as well as the terms that are neutral -the ones shared.

The process consist of calculating the polarity of the documents in the corpus and then use this to subset it into the two new artificial corpora. These two sets are then used to produce the word cloud visualizations for union and intersection of terms. 

```{r loading the data, message=F, warning=F, results="hide", echo=T}
if (file.exists(here("Chapter_4_Sentiment_Scoring","bos_airbnb_1k.rds"))) {
  bos.airbnb <- readRDS("bos_airbnb_1k.rds")
} else
{
  bos.airbnb <- read.csv(
    curl("https://raw.githubusercontent.com/padames/text_mining/master/bos_airbnb_1k.csv"),
    header = T, 
    stringsAsFactors = F)
  saveRDS(bos.airbnb, here("Chapter_4_Sentiment_Scoring","bos_airbnb_1k.rds"))
}
```

Now some data cleanup resusing our vectorized functions from Chapter 2 and a new function from `qdqp`
to remove non-ASCII characters. Also some proper names that appeared very often were added
to the stop words for removal.

```{r data cleanup}
host.names <- bos.airbnb$host_name
host.names <- mgsub("[^\x1F-\x7F]+", "", host.names, fixed = FALSE)
host.names <- mgsub("[(aA)nd]+|&+|/+", "", host.names, fixed = FALSE)
host.names <- host.names %>% 
  str_split(pattern = " ") %>% 
  unlist %>% 
  tolower()
custom.stopwords = c(stopwords('en'), host.names)

bos.airbnb$comments_original <- bos.airbnb$comments # preserve original
# qdap assumes "Text contains only ASCII characters"" so a thorough cleanup is needed
# Example of a document:"This place was very nice.<U+00A0> S<U+00E9>bastien was accommodating..."
# remove all non-ascii, regex pattern from https://stackoverflow.com/a/28001656/1585486
bos.airbnb$comments <- mgsub("[^\x1F-\x7F]+", "", bos.airbnb$comments, fixed = FALSE)
bos.airbnb$comments_ascii <- bos.airbnb$comments
# reuse our version of a vector clean up function from Chapter 2
bos.airbnb$comments <- e$clean.vec(bos.airbnb$comments, custom.stopwords)
bos.airbnb$comments <- strip(bos.airbnb$comments)
```

Now we can calulate the polarity list.

```{r polarity}
bos.pol <- polarity(bos.airbnb$comments) # a list with two members, one is a data frame called 'all', 
                                              # the polarities are in bos.polarity$all$polarity 
```

**Exploring the polarity data**

First let's plot the polarity distribution.

```{r polarity distribution, echo=TRUE}
p <- ggplot(data = bos.pol$all,
            mapping = aes(x = polarity, ..density..)) +
  theme_gdocs() +
  geom_histogram(binwidth = .15,
                 fill = "darkred", 
                 colour = "grey60",
                 size = .2) +
  geom_density(alpha = .25, fill = "white") +
  geom_text(aes( x = 2.75, y = 0.5,
                 label = paste(summary(as.data.frame(x = bos.pol$all$polarity))[1], 
                               summary(as.data.frame(x = bos.pol$all$polarity))[2],
                               summary(as.data.frame(x = bos.pol$all$polarity))[3],
                               summary(as.data.frame(x = bos.pol$all$polarity))[4],
                               summary(as.data.frame(x = bos.pol$all$polarity))[5],
                               summary(as.data.frame(x = bos.pol$all$polarity))[6], sep = "\n")))
p
```

We see very few negative words and a peak in polarity around `r round(mean(bos.airbnb$polarities), digits = 2)`. On average every Airbnb review gets a positive word due to response bias in trying to conform to a social norm of *politeness*. This produces *grade inflation* of the positive words inserted in excess around the negative ones. In order to avoid this bias it is recommended to use the `scale` function that centers and scales the data around 0 using the mean as central reference and the standard deviation for scaling.

```{r scaling polarity and its effects}
# create a new column in original data frame:
polarities_original <- summary(as.vector(bos.pol$all$polarity))
bos.pol$all$polarity <- scale(bos.pol$all$polarity) 
polarities_scaled <- summary(as.vector(x = bos.pol$all$polarity))
# table to compare before and after scaling and centering:
round(cbind(polarities_original, polarities_scaled), 2) 
```

Now lets subset the comments according to polarity.

```{r subsetting the comments by polarity}
pos.comments <- bos.airbnb[bos.pol$all$polarity > 0, "comments"]
neg.comments <- bos.airbnb[bos.pol$all$polarity < 0, "comments"]
```

```{r word cloud by polarity, message=F, warning=F, results="hide", echo=T}
library(wordcloud)
pos.terms <- paste(pos.comments, collapse = " ")
neg.terms <- paste(neg.comments, collapse = " ")
all.terms <- c(pos.terms, neg.terms)
all.corpus <- VCorpus(VectorSource(all.terms))
# read the book to get an explanation of what the TFIDF matrix is
all.tdm <- TermDocumentMatrix(x = all.corpus, control = list(weighting = weightTfIdf,
                                                             removePunctuation = TRUE,
                                                             stopwords = custom.stopwords))
all.tdm.m <- as.matrix(all.tdm)
colnames(all.tdm.m) <- c('positive', 'negative')
comparison.cloud(all.tdm.m, max.words = 100, colors = c('darkgreen', 'darkred'))
```

The positive wors seem related to snacks, maybe the ability to take public transit
close to the property, luxury features(?), good value, and a feel of
home. The negative seem related to cancelations from the property owner (?), lack of cleaningless,
and maybe the lack of elevators.


**Without scaling and using the TFIDF weighted TDM**

The results vary, first using the scaling of the polarities but not the special weighting
for common words that may appear in most of the documents.

```{r word cloud by polarity with scaling and TFIDF weighted TDM, message=F, warning=F,echo=T}
all.tdm.simple <- TermDocumentMatrix(x = all.corpus, control = list(weighting = weightTf,
                                                                    removePunctuation = TRUE,
                                                                    stopwords = custom.stopwords))
all.tdm.simple.m <- as.matrix(all.tdm.simple)
colnames(all.tdm.simple.m) <- c('positive', 'negative')
comparison.cloud(all.tdm.simple.m, max.words = 100, colors = c('darkgreen', 'darkred'))
```

One can see that words like `boston` score high with the simple TF-weighed TDM matrix. 
However the terms that show up seem to reinforce the positives as related to comfortable,
beautiful, quiet, safe, clean, spacious properties with friendly (welcoming?) hosts, 
useful and welcoming features, and good access.

The negatives seem related to property small size, lack of cleaningless, 
inconvenient location (far from transit perhaps?), noisy surroundings or appliances(?),
property access not simple (?) and problems with the reservation.

The results not scaling the polarity vector and using the simple TF-weighed TDM.

```{r word cloud by polarity without scaling and TFIDF weighted TDM, message=F, warning=F,echo=T}
pos.comments.simple <- bos.airbnb[bos.pol$all$polarity > 0, "comments"]
neg.comments.simple <- bos.airbnb[bos.pol$all$polarity < 0, "comments"]

pos.terms.simple <- paste(pos.comments.simple, collapse = " ")
neg.terms.simple <- paste(neg.comments.simple, collapse = " ")
all.terms.simple <- c(pos.terms.simple, neg.terms.simple)
all.corpus.simple <- VCorpus(VectorSource(all.terms.simple))


all.tdm.simple2 <- TermDocumentMatrix(x = all.corpus.simple, 
                                     control = list(weighting = weightTf,
                                                    removePunctuation = TRUE,
                                                    stopwords = custom.stopwords))
all.tdm.simple.m2 <- as.matrix(all.tdm.simple2)
colnames(all.tdm.simple.m2) <- c('positive', 'negative')
comparison.cloud(all.tdm.simple.m2, max.words = 100, colors = c('darkgreen', 'darkred'))
```

## Emoticons 

When using corpora from public social media chats or messege apps, an importan part of the
emotional content of the text comes sprinkled with emoticons and emojis. If one wants to add
their meaning into the sentiment analysis stream their identification and conversion into  
words and text is useful and even necessary.

The Airbnb reviews are an example of this.
There are as a minimum `r sum(stri_detect(str = bos.airbnb$comments_original, regex = ":-\\)|:>|:<|:-D|:\\(|;\\)|:\\)"))` punctuation-based emoticons and some
`r sum(stri_detect(str = bos.airbnb$comments_original,regex = "[^\x1F-\x7F]+"))` 
emojis in the original Airbnb reviews.

The first set of emoticons that need to be identified and transformed are the UTF-8 based ones
because their presence triggers errors like: **string 127 is invalid UTF-8** from many R functions.
Let's identify, isolate, convert to text and replace the emojis first and once they are transformed
into English words, we can proceed to identify and trnsform the punctuation-based emoticons.

First we load the file with the UTF-8 emoticon definitions and their corresponding R byte econding.
This information was found in https://raw.githubusercontent.com/today-is-a-good-day/emojis/master/emojis.csv.
The file was downloaded and a copy committed to this repo for redundancy. 

```{r loading emoji definition and encodings file}
if (file.exists(here("Chapter_4_Sentiment_Scoring","emoji.txt"))) {
  emoji_raw_df <- read.table(here("Chapter_4_Sentiment_Scoring", "emoji.txt"),
                             header = T, 
                             sep = ";")  %>%
  select(EN, tag, ftu8, unicode) %>% 
  rename(description = EN, r.encoding = ftu8)
} else {
  emoji_raw_df <- read.csv2(curl("https://raw.githubusercontent.com/today-is-a-good-day/emojis/master/emojis.csv")) %>%
  select(EN, tag, ftu8, unicode) %>% 
  rename(description = EN, r.encoding = ftu8)
}
# plain skin tones
skin_tones <- c("light skin tone", 
                "medium-light skin tone", 
                "medium skin tone",
                "medium-dark skin tone", 
                "dark skin tone")

# remove plain skin tones and remove skin tone info in description
emoji_df <- emoji_raw_df %>%
  # remove plain skin tones emojis
  filter(!description %in% skin_tones) %>%
  # remove emojis with skin tones info, e.g. remove woman: light skin tone and only keep woman
  filter(!grepl(":", description)) %>%
  mutate(description = tolower(description)) %>%
  mutate(unicode = as.u_char(unicode))
# all emojis with more than one unicode codepoint become NA 
```

The resulting data frame `emoji_df` has a column named `ftu8` that was renamed `r.encoding` following the 
book author preference. The important part is that the file contains the R byte encoding of each emoji.
The `tag` column has more descriptive words that may be used as a text replacement alternative, 
this can be compared to using the `EN` column which was renamed `description`, it has the standard English 
language description of the emoji.

The workflow involves first converting the punctuation-based emoticons to their equivalent English words
via `qdap`'s `mgsub` and the built-in dictionary named `emoticon`. However since applyig the `mgsub` fuction
to the vector of original comments threw the error:

```
> mgsub(pattern = emoticon[,2], replacement = emoticon[,1], bos.airbnb$comments_original)

Error in gsub(pattern[i], replacement[i], text.var, fixed = fixed, ...) : 
input string 127 is invalid UTF-8
```
A work around the first `ivalid UTF-8` error found in comment number 127 is
to turn all no-conformant UTF-8 Unicode codes into byte code.
This coversion does not affect valid UTF-8 entries that were processed by `mgsub` without throwing errors.

```{r punctuation-based emoticons, eval=TRUE, include=TRUE}
bos.airbnb$comments_original_to_byte <- iconv(bos.airbnb$comments_original, 
                                              from = "UTF-8", 
                                              to = "UTF-8", 
                                              sub = "byte")
bos.airbnb$comments_enhanced <- mgsub(pattern = emoticon[,2], 
                                       replacement = emoticon[,1], 
                                       bos.airbnb$comments_original_to_byte)
```

Then the UTF-8 encoding of the emojis can be converted to **R byte enconding**:

```{r converting emoji current text representation in UTF-8 block encoding to standard UTF-8}
bos.airbnb$comments_utf8 <- iconv(bos.airbnb$comments_original, from = "utf8", to = "utf8", sub = "byte")
```

Followed now by translating the R-byte found in the comments into English words that describe their meaning.
This is accomplished in the following block of R code:

```{r substituting emojis with text}
bos.airbnb$comments_enhanced <- mgsub(pattern = emoji_df$ftu8, 
                                       replacement = emoji_df$tag, 
                                       bos.airbnb$comments_utf8)
```

However a close inspection of the entries with non-ascii Unicode characters showed that there were no 
valid emojis to replace, only UTF-8 characters representing mostly entries in languages other than English.




