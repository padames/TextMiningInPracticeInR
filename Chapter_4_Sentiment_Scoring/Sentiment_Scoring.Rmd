---
title: "Sentiment Scoring"
author: "P. Adames"
date: "June 23, 2019"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Set Options
options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL', 'C')

library(stringi)
library(stringr)
library(qdap)
library(curl)
library(ggplot2)
library(ggthemes)
library(tm)
library(here) # read https://www.tidyverse.org/articles/2017/12/workflow-vs-script/

# package "here" assumes project file is in the root directory for relative paths to work
setwd(here("Chapter_4_Sentiment_Scoring"))

# helper functions
source(here("Chapter_2_Basics_of_Text_Mining", "Helpers.R"))
``` 

# What is sentiment analysis?

From Chapter 4 of the book: `Sentiment analysis is the process of extracting an author's emotional intent from text`. This entails using liguistics, psychology and natural languaje processing.
It is also a challenge to agree on the multiplicity of emotions and their cultural interpretation so there is always some degree of modelling/or analyst bias unless the specific intentions of the author are captured directly. Another reason for difficulty are contrasting sentiments on the same feature.

Among the numerous emotional frameworks there is one we will use created by a psychologist by the name Robert Plutchik in the 80's. It consists of eight primary emotions supposed to be essential for the survival of men and animals. Each has a polar opposite and can be expressed with different levels of intensity. Besides this other emotions can be expressed as a blend of two or more of these basic eigth.

## Basic emotions in Plutchik' emotion model

In parenthesis the polar opposites:

1. Anger (Joy)
2. Fear (Anger)
3. Sadness (Joy)
4. Disgust (Trust)
5. Surprise (Anticipation)
6. Anticipation (Surprise)
7. Trust (Disgust)
8. Joy (Sadness)

The complexity of using sentiment analysis becomes aparent considering that there
are many other frameworks for defining emotions. The main sourc of ambiguity and bias comes
from labeling a training set of n-grams and emotions from a particular sentiment framework.

## Polarity

It is simpler to analyse a text for sentiment polarity which in addition to identifying 
a sentiment it states whether it is positive or negative.

Three packages will be used in the exercises that follow:

Package             |  Use
--------------------|------------------------------------------------------------------
`sentiment`         | Basic sentiment analysis
`qdap`              | Polarity analysis
`tidytext`          | Sentiment scoring



# Sentiment Analysis: Parlor trick or insightful?

The result of sentiment scoring must be complemented with domain knowledge to make it useful in a business case. Otherwise the risk is accurate sentiment descriptions that don't lead to actionable decisions to improve a product or service. Just stating that a result is positive or negative does not make it useful.


_Example:_ Identify the positive and negative qualities of an Airbnb porperty in the Boston area.

Steps:

1. **Define problem and specific goal:** what property qualities are listed in positive/negative comments
2. **Identify the text thah needs to be collected:** the comments left by users of properties
3. **Organize text:** Select 1000 random Airbnb listing in the Boston area. Clean and organize in a frequency matrix
4. **Extract features:** Calculate sentiment and polarity scores
5. **Analyze:** Subset the comments according to sentiment and polarity in order to analyze the terms used for positive and negative comments.
6. **Make an insight or recommendation:**  Answering question formulated in step 1 will tell you if a property has the qualities considered positive in the Boston Area.


# Polarity: Simple sentiment scoring

Polarity is the positive or negative intent in the writer's tone. 
Here we will use the polarity function in the `qdap` package to do a simple but surprisingly accurate assessment of this condition. The result can be customized to specific needs by adjusting the subjectivity lexicon.

## Subjectivity lexicon

This is a list of words associated with a particular emotional state. The `qdap` function uses a peer-reviewd subjectivity lexicon from Bing Liu at the University of Illinois, consisting of 6,800 labeled words. 

A net polarity score can be calculated for a sentence by adding the positive or negative value of each word as per the subjectivity lexicon. The result is a negative, neutral, or positive tone for the excerpt. However this approach has to also consider amplifier and negaion words, so that an expression like "this is no good" results in a net negative value instead of neutral.

The lexicon can be customized to include terms specific to the text under analysis, to account for regionalisms, mannerisms, and so on.

Zipf's law states that a word will be used inversely proportional to its rank in the frequency list. That is, the number of times the fifth most frequent word will apear in a corpus with N terms will be approximately N/5.

This is explained by the least effort exerted by humans when trying to convey meaning in spite of the size of their vocabulary. A complicated message will promote disengagement from the other part, so a well researched lexicon will capture those most commonly used words relatively easily.


## `qdap` scoring for positive and negative word choice

The function works by identifying cluster of words and scoring magnifiers with +0.8 and negations with -0.8. Negative and positive words receive -1 or +1 respectively.
Additional Airbnb terms usually appearing in the comments must be added to the lexicon for more relevant results.

```{r polarity using custom lexicon terms for Airbnb}
# vector of new additional positive terms
new.pos <- c('rofl', 'lol') # rolling on the floor laughing and laugh out loud
all.pos <- c(new.pos, positive.words) # original and new positive terms
new.neg <- c('kappa','meh') # negative Airbnb terms for properties in Boston (from search)
all.neg <- c(new.neg, negative.words)  # original and new negative terms
all.polarity <- sentiment_frame(all.pos, all.neg)
polarity('ROFL, look at that!', polarity.frame = all.polarity)
```


Compare this polarity with the one obtained using the standard lexicon:

```{r polarity using standard lexicon}
polarity('ROFL, look at that!')
```

This proves the need to include terms specific to the text under study.


The same applies to the sentence `whatever you say, kappa`. Using the custom lexicon one gets a polarity result:

```{r second example of polarity using custom lexicon}
polarity('whatever you say, kappa', polarity.frame = all.polarity)
```

But using the standard lexicon:

```{r second example of polarity using standard lexicon}
polarity('whatever you say, kappa')
```

## Revisiting word clouds - Sentiment word clouds


Polarity analysis can be used to create two new corpora out of a single one and then study the terms that make positive or negative word clouds, as well as the terms that are neutral -the ones shared.

The process consist of calculating the polarity of the documents in the corpus and then use this to subset it into the two new artificial corpora. These two sets are then used to produce the word cloud visualizations for union and intersection of terms. 

```{r loading the data, message=F, warning=F, results="hide", echo=T}
if (file.exists(here("Chapter_4_Sentiment_Scoring","bos_airbnb_1k.rds"))) {
  bos.airbnb <- readRDS("bos_airbnb_1k.rds")
} else
{
  bos.airbnb <- read.csv(
    curl("https://raw.githubusercontent.com/padames/text_mining/master/bos_airbnb_1k.csv"),
    header = T, 
    stringsAsFactors = F)
  saveRDS(bos.airbnb, here("Chapter_4_Sentiment_Scoring","bos_airbnb_1k.rds"))
}
```

Now some data cleanup resusing our vectorized functions from Chapter 2 and a new function from `qdqp`
to remove non-ASCII characters. Also some proper names that appeared very often were added
to the stop words for removal.

```{r data cleanup}
host.names <- bos.airbnb$host_name
host.names <- mgsub("[^\x1F-\x7F]+", "", host.names, fixed = FALSE)
host.names <- mgsub("[(aA)nd]+|&+|/+", "", host.names, fixed = FALSE)
host.names <- host.names %>% 
  str_split(pattern = " ") %>% 
  unlist %>% 
  tolower()
custom.stopwords = c(stopwords('en'), host.names)

bos.airbnb$comments_original <- bos.airbnb$comments # preserve original
# qdap assumes "Text contains only ASCII characters"" so a thorough cleanup is needed
# Example of a document:"This place was very nice.<U+00A0> S<U+00E9>bastien was accommodating..."
# remove all non-ascii, regex pattern from https://stackoverflow.com/a/28001656/1585486
bos.airbnb$comments <- mgsub("[^\x1F-\x7F]+", "", bos.airbnb$comments, fixed = FALSE)
# reuse our version of a vector clean up function from Chapter 2
bos.airbnb$comments <- e$clean.vec(bos.airbnb$comments, custom.stopwords)
bos.airbnb$comments <- strip(bos.airbnb$comments)
```

Now we can calulate the polarity list.

```{r polarity}
bos.airbnb$polarity <- polarity(bos.airbnb$comments) # a list with two members, one is a data frame called 'all', 
                                              # the polarities are in bos.polarity$all$polarity 
bos.airbnb$polarities <- bos.airbnb$polarity$all$polarity
```

**Exploring the polarity data**

First let's plot the polarity distribution.

```{r polarity distribution, echo=TRUE}
p <- ggplot(data = bos.airbnb,
            mapping = aes(x = polarities, ..density..)) +
  theme_gdocs() +
  geom_histogram(binwidth = .15,
                 fill = "darkred", 
                 colour = "grey60",
                 size = .2) +
  geom_density(alpha = .25, fill = "white") +
  geom_text(aes( x = 2.5, y = 0.5,
                 label = paste(summary(as.data.frame(x = bos.airbnb$polarities))[1], 
                               summary(as.data.frame(x = bos.airbnb$polarities))[2],
                               summary(as.data.frame(x = bos.airbnb$polarities))[3],
                               summary(as.data.frame(x = bos.airbnb$polarities))[4],
                               summary(as.data.frame(x = bos.airbnb$polarities))[5],
                               summary(as.data.frame(x = bos.airbnb$polarities))[6], sep = "\n")))
p
```

We see very few negative words and a peak in polarity around `r round(mean(bos.airbnb$polarities), digits = 2)`. On average every Airbnb review gets a positive word due to response bias in trying to conform to a social norm of *politeness*. This produces *grade inflation* of the positive words inserted in excess around the negative ones. In order to avoid this bias it is recommended to use the `scale` function that centers and scales the data around 0 using the mean as central reference and the standard deviation for scaling.

```{r scaling polarity and its effects}
# create a new column in original data frame:
polarities_original <- summary(as.vector(bos.airbnb$polarities))
bos.airbnb$polarities <- scale(bos.airbnb$polarities) 
polarities_scaled <- summary(as.vector(x = bos.airbnb$polarities))
# table to compare before and after scaling and centering:
round(cbind(polarities_original, polarities_scaled), 2) 
```

Now lets subset the comments according to polarity.

```{r subsetting the comments by polarity}
pos.comments <- bos.airbnb[bos.airbnb$polarities > 0, "comments"]
neg.comments <- bos.airbnb[bos.airbnb$polarities < 0, "comments"]
```

```{r word cloud by polarity, message=F, warning=F, results="hide", echo=T}
library(wordcloud)
pos.terms <- paste(pos.comments, collapse = " ")
neg.terms <- paste(neg.comments, collapse = " ")
all.terms <- c(pos.terms, neg.terms)
all.corpus <- VCorpus(VectorSource(all.terms))
# read the book to get an explanation of what the TFIDF matrix is
all.tdm <- TermDocumentMatrix(x = all.corpus, control = list(weighting = weightTfIdf,
                                                             removePunctuation = TRUE,
                                                             stopwords = custom.stopwords))
all.tdm.m <- as.matrix(all.tdm)
colnames(all.tdm.m) <- c('positive', 'negative')
comparison.cloud(all.tdm.m, max.words = 100, colors = c('darkgreen', 'darkred'))
```

