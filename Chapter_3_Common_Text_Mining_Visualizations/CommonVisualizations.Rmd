---
title: "Common Text Mining Visualizations"
author: "P. Adames"
date: "June 3, 2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Set Options
options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL', 'C')

library(stringi)
library(stringr)
library(qdap)
library(curl)
library(ggplot2)
library(ggthemes)
library(tm)

source("../Chapter_2_Basics_of_Text_Mining/Helpers.R")
```
## Text Frequency revisited

Finding the most frequent terms across all documents in a bag of words analysis is the first step
towards finding unusual features or to start exploring the reason for expected terms via associations.
This analysis guides subsequent text mining efforts focusig on relevant terms instead of on perhaps intersting outliers but irrelevant nonetheless.  

To solve the exercises from this chapter we will be reusing the code to clean up a corpus of text 
from Chapter 2: Basics of Text Mining. How effective data communicates meaning to any given 
audiece is a strong function of the skills and habits when it comes to consuming and interpreting information. This may have a decisive impact on the decisions made based on the data.

To give a concrete example of this, we will contrast the table of top 20 most frequent terms in the 
corpus of Delta Airlines Customer Service Tweets presented in the previous chapter with a plot of the same data.

```{r summary of cleaup code and term frequency data frame}
text.df <- read.csv(curl("https://raw.githubusercontent.com/padames/text_mining/master/oct_delta.csv"),
                    header = T, stringsAsFactors = F)
df <- data.frame(doc_id = seq(1:nrow(text.df)), # a numeric index
                 text = text.df$text) # the vector of 1377 tweets
corpus <- VCorpus(DataframeSource(df))
corpus.clean <- clean.corpus(corpus)
tdm <- TermDocumentMatrix(corpus.clean)
tdm.m <- as.matrix(tdm)
term.freq <- rowSums(tdm.m) # the matrix stores the words in an attribute list called dimnames
frequencies.df <- data.frame( word = names(term.freq), frequency = term.freq)
frequencies.df <- frequencies.df[order(term.freq, decreasing = T), ]
```

Now we can write the new code to build the `ggplot` visualization, notice the use of `geom_col` instead
of `geom_bar(stat="identity")` as per the original book example:

```{r ggplot visualization}
frequencies.df$word <- factor(x = frequencies.df$word, 
                              levels = unique(as.character(frequencies.df$word)))
ggplot(frequencies.df[1:20,], # pass data frame to create visualization from
       aes(x = word, y = frequency)) + # define the aesthetics as specific columns of the data frame
  geom_col(fill = "#FF9999", colour = "black") + # bars from the values of x (without transformations)
  coord_flip() +  # exchanges position of x and y
  theme_gdocs() + # predefined visual style for the plot mimicking Google Documents
  geom_text(aes(label = frequency), colour = "black", hjust = 1.25, size = 5.0 ) # labels for bars
```

Some parsing errors are apparent, for example the amp word is likely the \& symbol. Also it is confirmed
that most of the tweets are discussions and apologies about flight and confirmation numbers.


## Word Associations

From the previous result one can see that the word *apologies* appears quite often. This is expected given
the context of the DeltaAssit customer service airline context, however a valid question might be what are the agents apologizing for.

Finding the correlation between the incidence vectors of two words is a measure of how associated their presence is per document in the corpus of documents. The `tm` package defines function `findAssocs` to do just this.
[This stackoverflow answer](https://stackoverflow.com/a/43192064/1585486) sheds some ligth on the inner working of the function:

> The math of findAssoc() is based on the standard function cor() in the stats package of R.
> Given two numeric vectors, cor() computes their covariance divided by both the standard deviations.

Let's try to answer the question using the function `findAssocs` with a threshold value of 0.11.
This value depends on the corpus and the specific word, I suggest you try with higher values initially.

```{r findAsscocs use}
associations <- findAssocs(x = tdm, terms = 'apologies', corlimit = 0.11) # returns a list
associations <- as.data.frame(associations) # converted to a data frame with one vector for a column 
associations$terms <- row.names(x = associations) # adds new vector colomn of characters
associations$terms <- factor(x = associations$terms, # turns the new column unequivocally into factors
                             levels = associations$terms)  
associations <- associations[order(associations$terms,decreasing = T), ] # by decreasing association 
```

Now let's build the visualization Kwartler shows, here with reproducible code:

```{r word association visualization}
ggplot(data = associations, mapping = aes(y = terms)) +
  geom_point(data = associations, mapping = aes(x = apologies), size  = 5, colour = "#FF9999") +
  theme_gdocs() +
  geom_text(mapping = aes(x = apologies, label = apologies), colour = 'darkred', vjust = 2.0, size = 3) +
  theme(text = element_text(size = 15), axis.title.y = element_blank())
```

The term *apologies* correlates most strongly with the words *delay* and *issues*. Also *latearriving*  appears among the third most associated words. The word *refund* appears among the 5 words closest to the 
minimum threashold value set to find associations. 
One may infer based on these limited findings that there is evidence that Delta Airlines customer service
agents do apologize for issues causing delays in arrivals and may offer refunds to folow up the apology.

These findings may trigger more questions about how often these events happen and what consequences 
bring to the customer satisfaction and the bottom line of the company.


## Word networks

To try to understand how the refunds are made based on the Tweets from Delta Assist, let's subset them to the ones containing the word `refund`.

```{r subseting refunds}
refund.tweets <- df[grep("refund", df$text, ignore.case = T), ]
refund.corpus <- VCorpus(DataframeSource(refund.tweets))
refund.corpus <- clean.corpus(refund.corpus)
refund.tdm <- TermDocumentMatrix(refund.corpus)
```

There are `r nrow(refund.tweets)` tweets containing the word `refund`.
The previous code shows the creation of a new clean corpus and the term-document-matrix, these are necessary to build the visualization of the word network. 

The next step is to build the adjacency matrix of terms with respect to the documents. This matrix represents the number of documents where the terms appear together with other terms. 

```{r,  adjacency matrix, message=F}
library(igraph)
refund.matrix <- as.matrix(refund.tdm)
refund.frequencies <- rowSums(refund.matrix)
refund.matrix <- refund.matrix[order(refund.frequencies, decreasing = T), ]
MAX_WORD_COUT <- 42
refund.matrix <- refund.matrix[1:MAX_WORD_COUT, ]
refund.adj.matrix <- refund.matrix %*% t(refund.matrix) # %*% is the matrix multiplication operator
refund.adj.matrix <- graph.adjacency(refund.adj.matrix, 
                                     weighted = T, 
                                     mode = "undirected", 
                                     diag = T)
refund.adj.matrix <- simplify(graph = refund.adj.matrix)
```

Now to plot the graph:
hey 
```{r plot of word network, fig.width=8, fig.height=6}
plot.igraph(refund.adj.matrix, 
            vertex.shape = "none", 
            vertex.label.font =  1,
            vertex.label.color = "darkred",
            vertex.label.cex = .7,
            edge.color = "gray85")
title(main = '@DeltaAssist Refund Word Network')
```

An easier approach is to use the library `qdap` as illustrated by the following snippet.

```{r qdap word network, fig.width=8, fig.height=8}
library(qdap)
word_network_plot(refund.tweets$text[1:3])
title(main = '@DeltaAssist Refund Word Network')
```

The library `qpad` provides another way to generate word netwworks from the unfiltered data frame. Notice that al the clean up is done by the function itself.

```{r more control through qdap, echo=T, message=F, warning=F, fig.height=8, fig.width=10}
g <- word_associate(df$text,
                    match.string = c('refund'),
                    stopwords = Top200Words,
                    network.plot = T,
                    cloud.colors = c('gray85', 'darkred'))
title(main = '@DeltaAssist Refund Word Network')
```

These word networks do provide some insight but in order to produce useful visualizations the data set has to be small and thus its power decreases. They remain an exploratory tool to be used early on in text analysis, together with other exploratory strategies, but seldom to be used for final reporting.


# Simple word clusters: hierarchical dendrograms

These tree-like visualizations are a good early exploratory tool, also appropriate for small data sets.
The author recommends around 50 terms per visualization.
The technique involves calculating a distance matrix on the term document matrix, TDM, and using those distances between term vectors to be the criteria to start clustering terms. Starting with individual terms the algorithm starts pairing each term with its closest one until the main stem of the tree is reached.

```{r dendogram}
tdm_dense <- removeSparseTerms(x = tdm, sparse = 0.975) 

```

In order to avoid having to handle huge matrices, first a big proportion of the zeros in the TDM are removed via the `removeSparseTerms` function from the package `tm`. This leaves `r nrow(as.matrix(tdm_dense))` terms instead of the original `r nrow(as.matrix(tdm))`. The `sparse` argument indicates the maximum fraction of zeros to be removed from the term vectors, in this case up to 97.5% of the empty slots are removed in each vector.

```{r hierarchical clustering creation}
hc <- hclust(d = dist(tdm_dense,
                      method = "euclidean"),
             method = "complete")
```
Then the clustering object is created with the `hc` function that takes the distance matrix.

```{r plotting the dendrogram, fig.height=8, fig.width=10}
plot(x = hc, 
     yaxt = 'n', 
     main = "@DeltaAssist Dendrogram")
```

Finally the dendogram is plotted and an obvious cluster related to a flight confirmation request is very apparent. Other smaller clusters can be observed around apologies and delay, but in general it will be hard to infer something more than the very obvious clusters. The proximity of these terms to each other is determined by the distance metric used, so the author recommend experimenting with it.

```{r hierarchical clustering creation different distance metric, fig.height=8, fig.width=10}
hc2 <- hclust(d = dist(tdm_dense,
                       method = "maximum"),
              method = "complete")
plot(x = hc2, 
     yaxt = 'n', 
     main = "@DeltaAssist Dendrogram")
```
Using the maximum distance between term vectors yields cleaner clusters, assigning meaning to these groups may be a little more opaque. As with other exploratory approaches one has to keep looking for the unexpected.


```{r circular dendrogram, message = F}
library(dendextend)
library(circlize)
hcd <- as.dendrogram(hc) %>% 
  color_labels(6, col = c('darkgrey', 'blue', '#bada55', 'darkgreen', 'red', "black")) %>%
  color_branches(6, col = c('darkgrey', 'blue', '#bada55', 'darkgreen', 'red', "black"))
circlize_dendrogram(hcd, labels_track_height = 0.5, dend_track_height = 0.4)
```

Using `dendextend` one assignes the number of clusters to the branches, via color assignment: from black, as the bottom most cluster, counter clock wise passing through red, green, light green, blue, and grey, as the top cluster.
There is a big cluster, shown in green that makes the analysis rather non-conclusive.

```{r circular dendrogram with maximum for distance, message = F}
hcd2 <- as.dendrogram(hc2) %>% 
  color_labels(6, col = c('darkgrey', 'blue', '#bada55', 'darkgreen', 'red', "black")) %>%
  color_branches(6, col = c('darkgrey', 'blue', '#bada55', 'darkgreen', 'red', "black"))
circlize_dendrogram(hcd2, labels_track_height = 0.5, dend_track_height = 0.4)
```

Using the dendogram created with the `maximum` as distance between term vectors, the distribution of cluster size gets worse as clearly seen in this visualization, with the cluster in black comprising the majority of the words.
As witht the previous technique, it may be useful for exploration to acquaint oneself with the data but rather ineffective by itself to conclude anyhting of significance.

```{r hierarchical clustering creation one more different distance metric, message = F}
hc3 <- hclust(d = dist(tdm_dense,
                       method = "canberra"),
              method = "complete")
hcd3 <- as.dendrogram(hc3) %>% 
  color_labels(6, col = c('darkgrey', 'blue', '#bada55', 'darkgreen', 'red', "black")) %>%
  color_branches(6, col = c('darkgrey', 'blue', '#bada55', 'darkgreen', 'red', "black"))
circlize_dendrogram(hcd3, labels_track_height = 0.5, dend_track_height = 0.4)
```

The [`canberra`](https://en.wikipedia.org/wiki/Canberra_distance) distance does provide a few more clusters. Some of them can be used for interpreting perhaps related subjects: one about positive flight feedback in grey, another related to bagagge service in blue, and yet another one around instructions to a ticket number in light green color. Of course further confirmation of the meaning of these clusters should follow.


# Word clouds

These are visualizations based on word frequency as the main dimension represented by font size: the larger the font the more frequent it appears in the corpora.
Other dimensions can be added by using colour and proximity to each other.
Its wide adoption due to ease of interpretation has led to its excessive and sometimes arguably inadequate use. Special care must be taken in the selection of specific stop words durig preprocessing as this can introduce significant bias.

The library `wordcloud` has three main functions: 

 Function name           | Purpose
 ------------------------|---------------------------------------------------------------------------
 `wordcloud`             | Show the absolute frequency of words in one corpus of text
 `commonality.cloud`     | Show the contrast of common word frequency among term vectors in two or more corpora
 `comparison.cloud`      | Show the contrast between dissimilar words amog term vectors in two or more corpora
 
 
## One corpus word clouds

Takes a vector of terms and a vector of term frequecies.

```{r one corpus word cloud}
library(wordcloud)
wordcloud(words = frequencies.df$word, 
          freq = frequencies.df$frequency, 
          max.words = 100, 
          min.freq = 20,  
          colors = c('black','darkred'))
```

Notice the only dimension represented in this visualization is term frequency.
The information displayed is the same one appearing in the table earlier in this document, however audiences usually consume this visualization much more readily. 

## Comparing and contrasting corpora in word clouds

In order to do this we need to import our second corpus:

```{r second corpus for comparisons, message = F}
# This url is my fork from the book author repo as of May 2019
amazn <- read.csv(curl("https://raw.githubusercontent.com/padames/text_mining/master/amzn_cs.csv"),
                  header = T, stringsAsFactors = F)
delta <- text.df #previously read from the original repository in this same document

# Override custom words based on previous work on the DeltaAssit corpus
custom.stopwords = c(stopwords('en'), 'sorry', 'amp', 'delta', 'amazon')

# Build the two text vectors to build the two corpora
amazn.vec <- clean.vec(amazn$text)
delta.vec <- clean.vec(delta$text) 

# Each vector is collapsed into a single document representing all the terms from each domain
amazn.vec <- paste(amazn.vec, collapse = " ")
delta.vec <- paste(delta.vec, collapse = " ")

# Build the new corpus with two corpora and its term-document-matrix (TDM)
corpus2 <- VCorpus(VectorSource(c(amazn.vec,delta.vec)))
tdm2 <- TermDocumentMatrix(corpus2)
tdm2.m <- as.matrix(tdm2)
colnames(tdm2.m) <- c("Amazon", "Delta") # capitalized to distinguish them from the actual terms
# a sampler of the corpus
tdm2.m[3480:3490,]
```

Now lets add the color palette and create the visualization.

```{r colour palette and word cloud, message=F}
library(RColorBrewer)
```

